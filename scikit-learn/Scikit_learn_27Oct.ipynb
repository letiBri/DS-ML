{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58e4854a-78b0-4486-8b77-d9095e54d5ac",
   "metadata": {},
   "source": [
    "# SCIKIT-LEARN 29/10 REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d6d10ba-2a69-4893-9cab-fb17c6ee2f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d61df3fe-3d80-4ef5-92c5-ee711e786559",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2070bb27-25e2-453c-a4cf-bf9857d278ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac89803-a3b2-4d9a-ac8a-b050a4ce318d",
   "metadata": {},
   "source": [
    "**REGRESSION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "614b67f4-f7d4-4432-9ae1-55e77587665d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "#Tipo: regressione\n",
    "#Target: livello di progressione del diabete dopo un anno\n",
    "#Feature: età, BMI, pressione, colesterolo ecc.\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, y = load_diabetes(return_X_y=True)  \n",
    "# divido in train e test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42)\n",
    "# divido i dati in 80% train e 20% test\n",
    "# stratify=y serve a mantenere la stessa proporzione di classi nei due insiemi\n",
    "# random_state=42 rende la divisione riproducibile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca6166a5-f8bf-4178-b406-eff44459f825",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression() # instance of the regressor\n",
    "reg.fit(X_train, y_train)  # addestro (fitto) il modello usando i dati di train\n",
    "y_test_pred = reg.predict(X_test)  # predico i valori di y per i dati di test\n",
    "# y_test_pred contiene le predizioni continue (numeriche) fatte dal modello sui campioni di test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68032d4f-31a8-4a84-9c5f-6c710e2b604e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([139.5475584 , 179.51720835, 134.03875572, 291.41702925,\n",
       "       123.78965872,  92.1723465 , 258.23238899, 181.33732057,\n",
       "        90.22411311, 108.63375858,  94.13865744, 168.43486358,\n",
       "        53.5047888 , 206.63081659, 100.12925869, 130.66657085,\n",
       "       219.53071499, 250.7803234 , 196.3688346 , 218.57511815,\n",
       "       207.35050182,  88.48340941,  70.43285917, 188.95914235,\n",
       "       154.8868162 , 159.36170122, 188.31263363, 180.39094033,\n",
       "        47.99046561, 108.97453871, 174.77897633,  86.36406656,\n",
       "       132.95761215, 184.53819483, 173.83220911, 190.35858492,\n",
       "       124.4156176 , 119.65110656, 147.95168682,  59.05405241,\n",
       "        71.62331856, 107.68284704, 165.45365458, 155.00975931,\n",
       "       171.04799096,  61.45761356,  71.66672581, 114.96732206,\n",
       "        51.57975523, 167.57599528, 152.52291955,  62.95568515,\n",
       "       103.49741722, 109.20751489, 175.64118426, 154.60296242,\n",
       "        94.41704366, 210.74209145, 120.2566205 ,  77.61585399,\n",
       "       187.93203995, 206.49337474, 140.63167076, 105.59678023,\n",
       "       130.70432536, 202.18534537, 171.13039501, 164.91423047,\n",
       "       124.72472569, 144.81030894, 181.99635452, 199.41369642,\n",
       "       234.21436188, 145.95665512,  79.86703276, 157.36941275,\n",
       "       192.74412541, 208.89814032, 158.58722555, 206.02195855,\n",
       "       107.47971675, 140.93598906,  54.82129332,  55.92573195,\n",
       "       115.01180018,  78.95584188,  81.56087285,  54.37997256,\n",
       "       166.2543518 ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2e3bac-27e7-4c0a-97be-e1a07edbefa2",
   "metadata": {},
   "source": [
    "**Evaluation metrics for regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7eff5e12-5b35-43f0-a3c8-57a50a67455c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# Compute R2, MAE and MSE:\n",
    "r2 = r2_score(y_test, y_test_pred) # quanto bene il modello spiega i dati\n",
    "mae = mean_absolute_error(y_test, y_test_pred) # errore medio in val assoluto\n",
    "mse = mean_squared_error(y_test, y_test_pred) # penalizza di più gli errori grandi"
   ]
  },
  {
   "cell_type": "raw",
   "id": "59c8ab99-876c-4960-a885-8978abad0f4c",
   "metadata": {},
   "source": [
    "R² (coefficiente di determinazione): misura quanto bene il modello spiega la variabilità di y.\n",
    "Valori vicini a 1 -> ottimo modello; valori negativi -> modello peggiore di una media costante.\n",
    "\n",
    "MAE (Mean Absolute Error): media degli errori assoluti |y_true - y_pred|.\n",
    "Misura l’errore medio in “unità” del target, quindi è facile da interpretare.\n",
    "\n",
    "MSE (Mean Squared Error): media dei quadrati degli errori (y_true - y_pred)².\n",
    "Penalizza di più gli errori grandi rispetto al MAE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36b2111-ab5f-42c8-aec6-a1e6c3372f20",
   "metadata": {},
   "source": [
    "**Evaluation with cross_val_score()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f14e941b-1663-4fc5-afe4-0fca17390acc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.42955615, 0.52259939, 0.48268054, 0.42649776, 0.55024834])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "reg = LinearRegression()\n",
    "r2 = cross_val_score(reg, X, y, cv=5, scoring='r2')\n",
    "r2"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f6b506ed-e071-43a7-9c7b-1cdf457adf07",
   "metadata": {},
   "source": [
    "# eseguo la cross-validation a 5 fold (cv=5)\n",
    "# il dataset viene diviso in 5 parti (folds):\n",
    "#   - ogni volta il modello viene addestrato su 4/5 dei dati\n",
    "#   - e testato sul 1/5 rimanente\n",
    "# 'scoring=\"r2\"' indica che viene calcolato l'R² (coefficiente di determinazione)\n",
    "# il risultato è un array con 5 valori di R² (uno per ciascun fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8268624a-a776-4cea-9bf1-ded805bc1073",
   "metadata": {},
   "source": [
    "**POlYNOMIAL REGRESSION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec09ea6a-d6c9-48f9-a2d2-dac231a3a688",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(5) # costruisco polinomio con grado massimo a 5\n",
    "X_poly = poly.fit_transform(X) # trasformiamo i dati ma devo prima aver fatto il fit del modello"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1aaa2594-7fdc-4569-9368-64f5b6db34ae",
   "metadata": {},
   "source": [
    "poly = PolynomialFeatures(5)\n",
    "# creo un oggetto PolynomialFeatures che costruirà feature fino al 5° grado\n",
    "# es: se hai una feature x, genera [1, x, x², x³, x⁴, x⁵]\n",
    "\n",
    "X_poly = poly.fit_transform(X)\n",
    "# \"fit_transform\" fa due cose:\n",
    "# .fit() → calcola come creare le nuove combinazioni di feature\n",
    "# .transform() → applica la trasformazione ai tuoi dati X\n",
    "# il risultato X_poly è una nuova matrice con molte più colonne (feature)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "439dc6fa-e6e0-4d1f-97a8-99de527232b2",
   "metadata": {},
   "source": [
    "degree = se il grado viene specificato come tupla allora sto passando il grado massimo e minimo. Posso specificarlo anche come intero e indica solo il massimo.\n",
    "interaction_only = se vale True allora include le interazioni tra le feature ma esclude quelle con interazioni di grado maggiore di 2\n",
    "include_bias = se è true allora aggiunge una colonna bias \n",
    "\n",
    "otuput del fit_trasform è un array 2Dnumpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524ba5f8-60fb-406b-8fb2-afdce36f8450",
   "metadata": {},
   "source": [
    "**PIPELINE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af73a779-c663-4b84-af82-33c6d53cc689",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline # pipline --> catena di step consecutivi\n",
    "reg = make_pipeline(PolynomialFeatures(5), LinearRegression())\n",
    "reg.fit(X_train, y_train)\n",
    "y_test_pred = reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dce3f634-edb3-4b6e-b870-203bacb7a834",
   "metadata": {},
   "source": [
    "reg = make_pipeline(PolynomialFeatures(5), LinearRegression())\n",
    "# creo una pipeline con due passaggi:\n",
    "# PolynomialFeatures(5): trasforma X aggiungendo tutte le combinazioni polinomiali fino al grado 5\n",
    "# LinearRegression(): addestra un modello lineare sulle feature trasformate\n",
    "# in pratica: regressione polinomiale di grado 5\n",
    "\n",
    "reg.fit(X_train, y_train)\n",
    "# esegue automaticamente:\n",
    "# - il \"fit\" di PolynomialFeatures sul training set\n",
    "# - la trasformazione di X_train in X_train_polynomial\n",
    "# - l’addestramento della LinearRegression su queste nuove feature\n",
    "\n",
    "y_test_pred = reg.predict(X_test)\n",
    "# esegue automaticamente la trasformazione polinomiale su X_test\n",
    "# poi usa il modello addestrato per predire i valori di y"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8959ad8b-b83d-456f-a05f-4bef83f8b4d6",
   "metadata": {},
   "source": [
    "pipeline sono sequenze di operazioni dove tutti gli elementi della sequenza tranne l'ultimo sono le trasformazioni, l'ultimo è il predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3c48c8-9256-4509-acd9-21c66458818f",
   "metadata": {},
   "source": [
    "**RIDGE AND LASSO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6718af29-b70f-45c2-a31d-1ad3c869be93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "reg = Ridge(alpha=0.5) \n",
    "# creo un modello di Ridge Regression\n",
    "# 'alpha' controlla l'intensità della penalità sui coefficienti (termine di regularizzazione)\n",
    "# maggiore è alpha → più forte la penalizzazione → i coefficienti vengono \"ristretti\" verso 0\n",
    "# alpha=0 → equivale alla regressione lineare normale (senza regularization)\n",
    "\n",
    "# dopo aver creato reg, puoi addestrarlo come sempre"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ac0ae885-6b67-4b68-b9c0-5c2c13ae9ff3",
   "metadata": {},
   "source": [
    "Ridge:\n",
    "- riduce l’impatto di feature con coefficienti troppo grandi\n",
    "- migliora la stabilità e la generalizzazione del modello\n",
    "- utile quando le feature sono correlate tra loro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58bd1633-6ebe-446b-bc9f-3c5e41b0b752",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "reg = Lasso(alpha=0.5)\n",
    "# creo un modello Lasso con coefficiente di penalità alpha = 0.5\n",
    "# alpha controlla quanto \"forte\" è la penalizzazione sui coefficienti\n",
    "# maggiore è alpha → più i coefficienti vengono spinti verso 0\n",
    "\n",
    "# dopo aver creato reg, puoi addestrarlo come sempre"
   ]
  },
  {
   "cell_type": "raw",
   "id": "24dedd14-f24e-4eda-b09d-53c094b3a41f",
   "metadata": {},
   "source": [
    "La penalità L1 (Lasso) può portare alcuni coefficienti esattamente a 0.\n",
    "Significa che il modello elimina automaticamente alcune feature\n",
    "→ quindi Lasso fa anche feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57348fd2-3f33-4a61-a128-5d40aa31d51d",
   "metadata": {},
   "source": [
    "**HYPERPARAMETERS SELECTION**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f37e0510-3d60-4fef-95a1-07d42393a0c2",
   "metadata": {},
   "source": [
    "I superparametri sono scelti dall'utente\n",
    "Tipicamente facciamo un validation set che non è nè il test set nè il training set. E' un terzo set (VALIDATION SET) che usiamo per trovare i best hyperparameters\n",
    "Proviamo i modelli diverse volte sul validation set e valutiamo le performance. Scegliamo la configurazione che ci dà le best performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0003fb9-9223-4266-b4ae-364c4188f879",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "# importo la classe che crea una \"griglia\" di tutte le combinazioni possibili\n",
    "# tra i valori degli iperparametri che voglio testare\n",
    "params = {\"criterion\": ['gini', 'entropy', 'log_loss'],\n",
    "         \"max_depth\":list(range(1,100,20)), \n",
    "         \"min_impurity_decrease\": np.linspace(0.1, 1, 10)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5389ccc-fc78-4e5f-a5ce-b8269be35122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definisco il dizionario dei parametri da esplorare:\n",
    "# criterion → misura per scegliere le divisioni negli alberi decisionali:\n",
    "#    'gini'        = indice di Gini\n",
    "#    'entropy'     = basata sull’entropia (information gain)\n",
    "#    'log_loss'    = versione probabilistica dell’entropia\n",
    "# max_depth → profondità massima dell’albero\n",
    "#    qui viene testata con valori [1, 21, 41, 61, 81]\n",
    "# min_impurity_decrease → soglia minima per accettare una divisione\n",
    "#    np.linspace(0.1, 1, 10) genera 10 valori da 0.1 a 1.0\n",
    "#    più alto → l’albero è più “prudente” nel dividere → meno complesso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf96b2ac-467a-4b8b-8fe2-968d3503cab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'criterion': 'gini',\n",
       "  'max_depth': 1,\n",
       "  'min_impurity_decrease': np.float64(0.1)},\n",
       " {'criterion': 'gini',\n",
       "  'max_depth': 1,\n",
       "  'min_impurity_decrease': np.float64(0.2)},\n",
       " {'criterion': 'gini',\n",
       "  'max_depth': 1,\n",
       "  'min_impurity_decrease': np.float64(0.30000000000000004)},\n",
       " {'criterion': 'gini',\n",
       "  'max_depth': 1,\n",
       "  'min_impurity_decrease': np.float64(0.4)},\n",
       " {'criterion': 'gini',\n",
       "  'max_depth': 1,\n",
       "  'min_impurity_decrease': np.float64(0.5)},\n",
       " {'criterion': 'gini',\n",
       "  'max_depth': 1,\n",
       "  'min_impurity_decrease': np.float64(0.6)},\n",
       " {'criterion': 'gini',\n",
       "  'max_depth': 1,\n",
       "  'min_impurity_decrease': np.float64(0.7000000000000001)},\n",
       " {'criterion': 'gini',\n",
       "  'max_depth': 1,\n",
       "  'min_impurity_decrease': np.float64(0.8)},\n",
       " {'criterion': 'gini',\n",
       "  'max_depth': 1,\n",
       "  'min_impurity_decrease': np.float64(0.9)},\n",
       " {'criterion': 'gini',\n",
       "  'max_depth': 1,\n",
       "  'min_impurity_decrease': np.float64(1.0)},\n",
       " {'criterion': 'gini',\n",
       "  'max_depth': 21,\n",
       "  'min_impurity_decrease': np.float64(0.1)},\n",
       " {'criterion': 'gini',\n",
       "  'max_depth': 21,\n",
       "  'min_impurity_decrease': np.float64(0.2)},\n",
       " {'criterion': 'gini',\n",
       "  'max_depth': 21,\n",
       "  'min_impurity_decrease': np.float64(0.30000000000000004)},\n",
       " {'criterion': 'gini',\n",
       "  'max_depth': 21,\n",
       "  'min_impurity_decrease': np.float64(0.4)},\n",
       " {'criterion': 'gini',\n",
       "  'max_depth': 21,\n",
       "  'min_impurity_decrease': np.float64(0.5)},\n",
       " {'criterion': 'gini',\n",
       "  'max_depth': 21,\n",
       "  'min_impurity_decrease': np.float64(0.6)},\n",
       " {'criterion': 'gini',\n",
       "  'max_depth': 21,\n",
       "  'min_impurity_decrease': np.float64(0.7000000000000001)},\n",
       " {'criterion': 'gini',\n",
       "  'max_depth': 21,\n",
       "  'min_impurity_decrease': np.float64(0.8)},\n",
       " {'criterion': 'gini',\n",
       "  'max_depth': 21,\n",
       "  'min_impurity_decrease': np.float64(0.9)},\n",
       " {'criterion': 'gini',\n",
       "  'max_depth': 21,\n",
       "  'min_impurity_decrease': np.float64(1.0)},\n",
       " {'criterion': 'gini',\n",
       "  'max_depth': 41,\n",
       "  'min_impurity_decrease': np.float64(0.1)},\n",
       " {'criterion': 'gini',\n",
       "  'max_depth': 41,\n",
       "  'min_impurity_decrease': np.float64(0.2)},\n",
       " {'criterion': 'gini',\n",
       "  'max_depth': 41,\n",
       "  'min_impurity_decrease': np.float64(0.30000000000000004)},\n",
       " {'criterion': 'gini',\n",
       "  'max_depth': 41,\n",
       "  'min_impurity_decrease': np.float64(0.4)},\n",
       " {'criterion': 'gini',\n",
       "  'max_depth': 41,\n",
       "  'min_impurity_decrease': np.float64(0.5)},\n",
       " {'criterion': 'gini',\n",
       "  'max_depth': 41,\n",
       "  'min_impurity_decrease': np.float64(0.6)},\n",
       " {'criterion': 'gini',\n",
       "  'max_depth': 41,\n",
       "  'min_impurity_decrease': np.float64(0.7000000000000001)},\n",
       " {'criterion': 'gini',\n",
       "  'max_depth': 41,\n",
       "  'min_impurity_decrease': np.float64(0.8)},\n",
       " {'criterion': 'gini',\n",
       "  'max_depth': 41,\n",
       "  'min_impurity_decrease': np.float64(0.9)},\n",
       " {'criterion': 'gini',\n",
       "  'max_depth': 41,\n",
       "  'min_impurity_decrease': np.float64(1.0)},\n",
       " {'criterion': 'gini',\n",
       "  'max_depth': 61,\n",
       "  'min_impurity_decrease': np.float64(0.1)},\n",
       " {'criterion': 'gini',\n",
       "  'max_depth': 61,\n",
       "  'min_impurity_decrease': np.float64(0.2)},\n",
       " {'criterion': 'gini',\n",
       "  'max_depth': 61,\n",
       "  'min_impurity_decrease': np.float64(0.30000000000000004)},\n",
       " {'criterion': 'gini',\n",
       "  'max_depth': 61,\n",
       "  'min_impurity_decrease': np.float64(0.4)},\n",
       " {'criterion': 'gini',\n",
       "  'max_depth': 61,\n",
       "  'min_impurity_decrease': np.float64(0.5)},\n",
       " {'criterion': 'gini',\n",
       "  'max_depth': 61,\n",
       "  'min_impurity_decrease': np.float64(0.6)},\n",
       " {'criterion': 'gini',\n",
       "  'max_depth': 61,\n",
       "  'min_impurity_decrease': np.float64(0.7000000000000001)},\n",
       " {'criterion': 'gini',\n",
       "  'max_depth': 61,\n",
       "  'min_impurity_decrease': np.float64(0.8)},\n",
       " {'criterion': 'gini',\n",
       "  'max_depth': 61,\n",
       "  'min_impurity_decrease': np.float64(0.9)},\n",
       " {'criterion': 'gini',\n",
       "  'max_depth': 61,\n",
       "  'min_impurity_decrease': np.float64(1.0)},\n",
       " {'criterion': 'gini',\n",
       "  'max_depth': 81,\n",
       "  'min_impurity_decrease': np.float64(0.1)},\n",
       " {'criterion': 'gini',\n",
       "  'max_depth': 81,\n",
       "  'min_impurity_decrease': np.float64(0.2)},\n",
       " {'criterion': 'gini',\n",
       "  'max_depth': 81,\n",
       "  'min_impurity_decrease': np.float64(0.30000000000000004)},\n",
       " {'criterion': 'gini',\n",
       "  'max_depth': 81,\n",
       "  'min_impurity_decrease': np.float64(0.4)},\n",
       " {'criterion': 'gini',\n",
       "  'max_depth': 81,\n",
       "  'min_impurity_decrease': np.float64(0.5)},\n",
       " {'criterion': 'gini',\n",
       "  'max_depth': 81,\n",
       "  'min_impurity_decrease': np.float64(0.6)},\n",
       " {'criterion': 'gini',\n",
       "  'max_depth': 81,\n",
       "  'min_impurity_decrease': np.float64(0.7000000000000001)},\n",
       " {'criterion': 'gini',\n",
       "  'max_depth': 81,\n",
       "  'min_impurity_decrease': np.float64(0.8)},\n",
       " {'criterion': 'gini',\n",
       "  'max_depth': 81,\n",
       "  'min_impurity_decrease': np.float64(0.9)},\n",
       " {'criterion': 'gini',\n",
       "  'max_depth': 81,\n",
       "  'min_impurity_decrease': np.float64(1.0)},\n",
       " {'criterion': 'entropy',\n",
       "  'max_depth': 1,\n",
       "  'min_impurity_decrease': np.float64(0.1)},\n",
       " {'criterion': 'entropy',\n",
       "  'max_depth': 1,\n",
       "  'min_impurity_decrease': np.float64(0.2)},\n",
       " {'criterion': 'entropy',\n",
       "  'max_depth': 1,\n",
       "  'min_impurity_decrease': np.float64(0.30000000000000004)},\n",
       " {'criterion': 'entropy',\n",
       "  'max_depth': 1,\n",
       "  'min_impurity_decrease': np.float64(0.4)},\n",
       " {'criterion': 'entropy',\n",
       "  'max_depth': 1,\n",
       "  'min_impurity_decrease': np.float64(0.5)},\n",
       " {'criterion': 'entropy',\n",
       "  'max_depth': 1,\n",
       "  'min_impurity_decrease': np.float64(0.6)},\n",
       " {'criterion': 'entropy',\n",
       "  'max_depth': 1,\n",
       "  'min_impurity_decrease': np.float64(0.7000000000000001)},\n",
       " {'criterion': 'entropy',\n",
       "  'max_depth': 1,\n",
       "  'min_impurity_decrease': np.float64(0.8)},\n",
       " {'criterion': 'entropy',\n",
       "  'max_depth': 1,\n",
       "  'min_impurity_decrease': np.float64(0.9)},\n",
       " {'criterion': 'entropy',\n",
       "  'max_depth': 1,\n",
       "  'min_impurity_decrease': np.float64(1.0)},\n",
       " {'criterion': 'entropy',\n",
       "  'max_depth': 21,\n",
       "  'min_impurity_decrease': np.float64(0.1)},\n",
       " {'criterion': 'entropy',\n",
       "  'max_depth': 21,\n",
       "  'min_impurity_decrease': np.float64(0.2)},\n",
       " {'criterion': 'entropy',\n",
       "  'max_depth': 21,\n",
       "  'min_impurity_decrease': np.float64(0.30000000000000004)},\n",
       " {'criterion': 'entropy',\n",
       "  'max_depth': 21,\n",
       "  'min_impurity_decrease': np.float64(0.4)},\n",
       " {'criterion': 'entropy',\n",
       "  'max_depth': 21,\n",
       "  'min_impurity_decrease': np.float64(0.5)},\n",
       " {'criterion': 'entropy',\n",
       "  'max_depth': 21,\n",
       "  'min_impurity_decrease': np.float64(0.6)},\n",
       " {'criterion': 'entropy',\n",
       "  'max_depth': 21,\n",
       "  'min_impurity_decrease': np.float64(0.7000000000000001)},\n",
       " {'criterion': 'entropy',\n",
       "  'max_depth': 21,\n",
       "  'min_impurity_decrease': np.float64(0.8)},\n",
       " {'criterion': 'entropy',\n",
       "  'max_depth': 21,\n",
       "  'min_impurity_decrease': np.float64(0.9)},\n",
       " {'criterion': 'entropy',\n",
       "  'max_depth': 21,\n",
       "  'min_impurity_decrease': np.float64(1.0)},\n",
       " {'criterion': 'entropy',\n",
       "  'max_depth': 41,\n",
       "  'min_impurity_decrease': np.float64(0.1)},\n",
       " {'criterion': 'entropy',\n",
       "  'max_depth': 41,\n",
       "  'min_impurity_decrease': np.float64(0.2)},\n",
       " {'criterion': 'entropy',\n",
       "  'max_depth': 41,\n",
       "  'min_impurity_decrease': np.float64(0.30000000000000004)},\n",
       " {'criterion': 'entropy',\n",
       "  'max_depth': 41,\n",
       "  'min_impurity_decrease': np.float64(0.4)},\n",
       " {'criterion': 'entropy',\n",
       "  'max_depth': 41,\n",
       "  'min_impurity_decrease': np.float64(0.5)},\n",
       " {'criterion': 'entropy',\n",
       "  'max_depth': 41,\n",
       "  'min_impurity_decrease': np.float64(0.6)},\n",
       " {'criterion': 'entropy',\n",
       "  'max_depth': 41,\n",
       "  'min_impurity_decrease': np.float64(0.7000000000000001)},\n",
       " {'criterion': 'entropy',\n",
       "  'max_depth': 41,\n",
       "  'min_impurity_decrease': np.float64(0.8)},\n",
       " {'criterion': 'entropy',\n",
       "  'max_depth': 41,\n",
       "  'min_impurity_decrease': np.float64(0.9)},\n",
       " {'criterion': 'entropy',\n",
       "  'max_depth': 41,\n",
       "  'min_impurity_decrease': np.float64(1.0)},\n",
       " {'criterion': 'entropy',\n",
       "  'max_depth': 61,\n",
       "  'min_impurity_decrease': np.float64(0.1)},\n",
       " {'criterion': 'entropy',\n",
       "  'max_depth': 61,\n",
       "  'min_impurity_decrease': np.float64(0.2)},\n",
       " {'criterion': 'entropy',\n",
       "  'max_depth': 61,\n",
       "  'min_impurity_decrease': np.float64(0.30000000000000004)},\n",
       " {'criterion': 'entropy',\n",
       "  'max_depth': 61,\n",
       "  'min_impurity_decrease': np.float64(0.4)},\n",
       " {'criterion': 'entropy',\n",
       "  'max_depth': 61,\n",
       "  'min_impurity_decrease': np.float64(0.5)},\n",
       " {'criterion': 'entropy',\n",
       "  'max_depth': 61,\n",
       "  'min_impurity_decrease': np.float64(0.6)},\n",
       " {'criterion': 'entropy',\n",
       "  'max_depth': 61,\n",
       "  'min_impurity_decrease': np.float64(0.7000000000000001)},\n",
       " {'criterion': 'entropy',\n",
       "  'max_depth': 61,\n",
       "  'min_impurity_decrease': np.float64(0.8)},\n",
       " {'criterion': 'entropy',\n",
       "  'max_depth': 61,\n",
       "  'min_impurity_decrease': np.float64(0.9)},\n",
       " {'criterion': 'entropy',\n",
       "  'max_depth': 61,\n",
       "  'min_impurity_decrease': np.float64(1.0)},\n",
       " {'criterion': 'entropy',\n",
       "  'max_depth': 81,\n",
       "  'min_impurity_decrease': np.float64(0.1)},\n",
       " {'criterion': 'entropy',\n",
       "  'max_depth': 81,\n",
       "  'min_impurity_decrease': np.float64(0.2)},\n",
       " {'criterion': 'entropy',\n",
       "  'max_depth': 81,\n",
       "  'min_impurity_decrease': np.float64(0.30000000000000004)},\n",
       " {'criterion': 'entropy',\n",
       "  'max_depth': 81,\n",
       "  'min_impurity_decrease': np.float64(0.4)},\n",
       " {'criterion': 'entropy',\n",
       "  'max_depth': 81,\n",
       "  'min_impurity_decrease': np.float64(0.5)},\n",
       " {'criterion': 'entropy',\n",
       "  'max_depth': 81,\n",
       "  'min_impurity_decrease': np.float64(0.6)},\n",
       " {'criterion': 'entropy',\n",
       "  'max_depth': 81,\n",
       "  'min_impurity_decrease': np.float64(0.7000000000000001)},\n",
       " {'criterion': 'entropy',\n",
       "  'max_depth': 81,\n",
       "  'min_impurity_decrease': np.float64(0.8)},\n",
       " {'criterion': 'entropy',\n",
       "  'max_depth': 81,\n",
       "  'min_impurity_decrease': np.float64(0.9)},\n",
       " {'criterion': 'entropy',\n",
       "  'max_depth': 81,\n",
       "  'min_impurity_decrease': np.float64(1.0)},\n",
       " {'criterion': 'log_loss',\n",
       "  'max_depth': 1,\n",
       "  'min_impurity_decrease': np.float64(0.1)},\n",
       " {'criterion': 'log_loss',\n",
       "  'max_depth': 1,\n",
       "  'min_impurity_decrease': np.float64(0.2)},\n",
       " {'criterion': 'log_loss',\n",
       "  'max_depth': 1,\n",
       "  'min_impurity_decrease': np.float64(0.30000000000000004)},\n",
       " {'criterion': 'log_loss',\n",
       "  'max_depth': 1,\n",
       "  'min_impurity_decrease': np.float64(0.4)},\n",
       " {'criterion': 'log_loss',\n",
       "  'max_depth': 1,\n",
       "  'min_impurity_decrease': np.float64(0.5)},\n",
       " {'criterion': 'log_loss',\n",
       "  'max_depth': 1,\n",
       "  'min_impurity_decrease': np.float64(0.6)},\n",
       " {'criterion': 'log_loss',\n",
       "  'max_depth': 1,\n",
       "  'min_impurity_decrease': np.float64(0.7000000000000001)},\n",
       " {'criterion': 'log_loss',\n",
       "  'max_depth': 1,\n",
       "  'min_impurity_decrease': np.float64(0.8)},\n",
       " {'criterion': 'log_loss',\n",
       "  'max_depth': 1,\n",
       "  'min_impurity_decrease': np.float64(0.9)},\n",
       " {'criterion': 'log_loss',\n",
       "  'max_depth': 1,\n",
       "  'min_impurity_decrease': np.float64(1.0)},\n",
       " {'criterion': 'log_loss',\n",
       "  'max_depth': 21,\n",
       "  'min_impurity_decrease': np.float64(0.1)},\n",
       " {'criterion': 'log_loss',\n",
       "  'max_depth': 21,\n",
       "  'min_impurity_decrease': np.float64(0.2)},\n",
       " {'criterion': 'log_loss',\n",
       "  'max_depth': 21,\n",
       "  'min_impurity_decrease': np.float64(0.30000000000000004)},\n",
       " {'criterion': 'log_loss',\n",
       "  'max_depth': 21,\n",
       "  'min_impurity_decrease': np.float64(0.4)},\n",
       " {'criterion': 'log_loss',\n",
       "  'max_depth': 21,\n",
       "  'min_impurity_decrease': np.float64(0.5)},\n",
       " {'criterion': 'log_loss',\n",
       "  'max_depth': 21,\n",
       "  'min_impurity_decrease': np.float64(0.6)},\n",
       " {'criterion': 'log_loss',\n",
       "  'max_depth': 21,\n",
       "  'min_impurity_decrease': np.float64(0.7000000000000001)},\n",
       " {'criterion': 'log_loss',\n",
       "  'max_depth': 21,\n",
       "  'min_impurity_decrease': np.float64(0.8)},\n",
       " {'criterion': 'log_loss',\n",
       "  'max_depth': 21,\n",
       "  'min_impurity_decrease': np.float64(0.9)},\n",
       " {'criterion': 'log_loss',\n",
       "  'max_depth': 21,\n",
       "  'min_impurity_decrease': np.float64(1.0)},\n",
       " {'criterion': 'log_loss',\n",
       "  'max_depth': 41,\n",
       "  'min_impurity_decrease': np.float64(0.1)},\n",
       " {'criterion': 'log_loss',\n",
       "  'max_depth': 41,\n",
       "  'min_impurity_decrease': np.float64(0.2)},\n",
       " {'criterion': 'log_loss',\n",
       "  'max_depth': 41,\n",
       "  'min_impurity_decrease': np.float64(0.30000000000000004)},\n",
       " {'criterion': 'log_loss',\n",
       "  'max_depth': 41,\n",
       "  'min_impurity_decrease': np.float64(0.4)},\n",
       " {'criterion': 'log_loss',\n",
       "  'max_depth': 41,\n",
       "  'min_impurity_decrease': np.float64(0.5)},\n",
       " {'criterion': 'log_loss',\n",
       "  'max_depth': 41,\n",
       "  'min_impurity_decrease': np.float64(0.6)},\n",
       " {'criterion': 'log_loss',\n",
       "  'max_depth': 41,\n",
       "  'min_impurity_decrease': np.float64(0.7000000000000001)},\n",
       " {'criterion': 'log_loss',\n",
       "  'max_depth': 41,\n",
       "  'min_impurity_decrease': np.float64(0.8)},\n",
       " {'criterion': 'log_loss',\n",
       "  'max_depth': 41,\n",
       "  'min_impurity_decrease': np.float64(0.9)},\n",
       " {'criterion': 'log_loss',\n",
       "  'max_depth': 41,\n",
       "  'min_impurity_decrease': np.float64(1.0)},\n",
       " {'criterion': 'log_loss',\n",
       "  'max_depth': 61,\n",
       "  'min_impurity_decrease': np.float64(0.1)},\n",
       " {'criterion': 'log_loss',\n",
       "  'max_depth': 61,\n",
       "  'min_impurity_decrease': np.float64(0.2)},\n",
       " {'criterion': 'log_loss',\n",
       "  'max_depth': 61,\n",
       "  'min_impurity_decrease': np.float64(0.30000000000000004)},\n",
       " {'criterion': 'log_loss',\n",
       "  'max_depth': 61,\n",
       "  'min_impurity_decrease': np.float64(0.4)},\n",
       " {'criterion': 'log_loss',\n",
       "  'max_depth': 61,\n",
       "  'min_impurity_decrease': np.float64(0.5)},\n",
       " {'criterion': 'log_loss',\n",
       "  'max_depth': 61,\n",
       "  'min_impurity_decrease': np.float64(0.6)},\n",
       " {'criterion': 'log_loss',\n",
       "  'max_depth': 61,\n",
       "  'min_impurity_decrease': np.float64(0.7000000000000001)},\n",
       " {'criterion': 'log_loss',\n",
       "  'max_depth': 61,\n",
       "  'min_impurity_decrease': np.float64(0.8)},\n",
       " {'criterion': 'log_loss',\n",
       "  'max_depth': 61,\n",
       "  'min_impurity_decrease': np.float64(0.9)},\n",
       " {'criterion': 'log_loss',\n",
       "  'max_depth': 61,\n",
       "  'min_impurity_decrease': np.float64(1.0)},\n",
       " {'criterion': 'log_loss',\n",
       "  'max_depth': 81,\n",
       "  'min_impurity_decrease': np.float64(0.1)},\n",
       " {'criterion': 'log_loss',\n",
       "  'max_depth': 81,\n",
       "  'min_impurity_decrease': np.float64(0.2)},\n",
       " {'criterion': 'log_loss',\n",
       "  'max_depth': 81,\n",
       "  'min_impurity_decrease': np.float64(0.30000000000000004)},\n",
       " {'criterion': 'log_loss',\n",
       "  'max_depth': 81,\n",
       "  'min_impurity_decrease': np.float64(0.4)},\n",
       " {'criterion': 'log_loss',\n",
       "  'max_depth': 81,\n",
       "  'min_impurity_decrease': np.float64(0.5)},\n",
       " {'criterion': 'log_loss',\n",
       "  'max_depth': 81,\n",
       "  'min_impurity_decrease': np.float64(0.6)},\n",
       " {'criterion': 'log_loss',\n",
       "  'max_depth': 81,\n",
       "  'min_impurity_decrease': np.float64(0.7000000000000001)},\n",
       " {'criterion': 'log_loss',\n",
       "  'max_depth': 81,\n",
       "  'min_impurity_decrease': np.float64(0.8)},\n",
       " {'criterion': 'log_loss',\n",
       "  'max_depth': 81,\n",
       "  'min_impurity_decrease': np.float64(0.9)},\n",
       " {'criterion': 'log_loss',\n",
       "  'max_depth': 81,\n",
       "  'min_impurity_decrease': np.float64(1.0)}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ParameterGrid(params)) # mi stampa tutte le combinazioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "87662360-baf7-4443-8692-28ac48755778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': ['gini', 'entropy', 'log_loss'],\n",
       " 'max_depth': [1, 21, 41, 61, 81],\n",
       " 'min_impurity_decrease': array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params # quali sono gli iperparametri e i valori che voglio combinare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7464f690-a39c-4095-8e53-a2fc9a9e63f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# con paramgrid facciamo tutte le combinazioni --> 150\n",
    "len(list(ParameterGrid(params))) # 3*5*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b3b550d5-43a1-4c3a-9efb-75c9b6613a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# costruisco un dataset\n",
    "X = np.random.random((1000, 10)) # 1000 campioni con 10 feature\n",
    "y = np.random.randint(0,10, 1000) # 1000 etichette casuali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "06cd03fb-2fcf-4458-b49f-82de95d8000a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_valid, X_test, y_train_valid, y_test = train_test_split(X, y, train_size=0.8)\n",
    "# divido il dataset in:\n",
    "# 80% per train+validation, 20% per test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "81e8ce9b-3cd4-4954-9564-46923652df21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 10)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "17f494f1-688a-4c97-80fd-3bf778321505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 10)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dccf2c4f-f80e-43da-bb88-1c1470f0fba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid, train_size=0.75)\n",
    "# divido ulteriormente l’80% del train+valid in:\n",
    "# 75% train e 25% validation\n",
    "# → quindi alla fine: 60% train, 20% validation, 20% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6732019d-a523-4507-9b68-aeebf02e12a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 10)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "74a43f8d-dee2-46e0-a696-b3ce22c14a15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 10)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "51f923c3-9f31-42cc-a504-749703f1e850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importo l’albero decisionale e la funzione per calcolare l’accuratezza\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracies = [] # creo una lista vuota per salvare l’accuratezza di ogni configurazione\n",
    "for config in ParameterGrid(params):  # ciclo su tutte le combinazioni di parametri definite nella griglia\n",
    "    #clf = DecisionTreeClassifier(criterion=config[\"criterion\"],\n",
    "                                # max_depth=config[\"max_depth\"],\n",
    "                                #min_impurity=config[\"min_impurity_decrease\"])\n",
    "    # creo il classificatore usando i parametri della configurazione corrente\n",
    "    clf = DecisionTreeClassifier(**config) # same as previous lines\n",
    "    clf.fit(X_train, y_train) # addestro il modello sui dati di train\n",
    "    acc = accuracy_score(y_valid, clf.predict(X_valid)) # calcolo l’accuratezza sul validation set\n",
    "    accuracies.append(acc) # aggiungo il punteggio alla lista delle accuratezze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "79904834-820b-4431-9d8b-eeba6224fc69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(accuracies) # restituisce l’indice della configurazione con la miglior accuratezza\n",
    "# cerco la best performance"
   ]
  },
  {
   "cell_type": "raw",
   "id": "612f5e3d-8271-4f31-ad1b-f2b59156b00b",
   "metadata": {},
   "source": [
    "Sto facendo un grid search manuale sugli iperparametri dell’albero decisionale.\n",
    "\n",
    "accuracies contiene l’accuratezza di ciascuna combinazione.\n",
    "\n",
    "np.argmax(accuracies) ti dice quale configurazione ha dato la miglior performance sul validation set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
